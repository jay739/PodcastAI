{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Pypdf tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIRFDakIBwfF",
        "outputId": "1fe2e06a-9fc1-4a2b-e236-f20a14465a5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting tools\n",
            "  Downloading tools-0.1.9.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytils (from tools)\n",
            "  Downloading pytils-0.4.3.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tools) (1.17.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from tools) (5.3.1)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tools, pytils\n",
            "  Building wheel for tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tools: filename=tools-0.1.9-py3-none-any.whl size=46730 sha256=717a6d24669aa035d3422d21dee109da0a4681d186a50d5251fab1f0c23b84b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/d8/9d/52ad6058db295741fe0b776c0fcfdb6670036acab59ce4ccfd\n",
            "  Building wheel for pytils (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytils: filename=pytils-0.4.3-py3-none-any.whl size=32806 sha256=c7058095fc63065970b614b7ff80401d1d49588d5d82b579a6d9403c19340ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/a7/be/135c0d4eaa74b54f43b5b0e0b30284b1c2081fe0581424408a\n",
            "Successfully built tools pytils\n",
            "Installing collected packages: pytils, Pypdf, tools\n",
            "Successfully installed Pypdf-5.4.0 pytils-0.4.3 tools-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "import traceback\n",
        "\n",
        "# --- PDF Handling Library ---\n",
        "try:\n",
        "    from pypdf import PdfReader\n",
        "    logging.info(\"Successfully imported pypdf.\")\n",
        "except ImportError:\n",
        "    logging.error(\"Failed to import 'pypdf'. Please install it: pip install pypdf\")\n",
        "    # Define a dummy class if import fails to allow script structure check\n",
        "    class PdfReader:\n",
        "        def __init__(self, *args): raise ImportError(\"pypdf not found\")\n",
        "        pages = []\n",
        "        metadata = None\n",
        "        is_encrypted = False\n",
        "    # exit() # Use exit() in a real script\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
        "\n",
        "# --- PDF Handling Functions (Independent) ---\n",
        "\n",
        "def get_pdf_file(pdf_folder: str, mode: str = \"latest\") -> str:\n",
        "    \"\"\"\n",
        "    Selects a PDF file path from a specified folder based on the mode.\n",
        "    (Standalone version)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdf_folder):\n",
        "        try:\n",
        "            os.makedirs(pdf_folder, exist_ok=True)\n",
        "            logging.warning(f\"PDF folder '{pdf_folder}' did not exist and was created.\")\n",
        "            raise FileNotFoundError(f\"PDF folder '{pdf_folder}' was created, but no PDFs found.\")\n",
        "        except OSError as e:\n",
        "            logging.error(f\"Failed to create PDF folder '{pdf_folder}': {e}\")\n",
        "            raise FileNotFoundError(f\"PDF folder '{pdf_folder}' does not exist and could not be created.\")\n",
        "\n",
        "    try:\n",
        "        pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf') and os.path.isfile(os.path.join(pdf_folder, f))]\n",
        "    except OSError as e:\n",
        "        logging.error(f\"Error listing files in folder '{pdf_folder}': {e}\")\n",
        "        raise FileNotFoundError(f\"Could not access files in the PDF folder '{pdf_folder}'.\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        logging.warning(f\"No PDF files found in folder: {pdf_folder}\")\n",
        "        raise FileNotFoundError(f\"No PDF files found in {pdf_folder}\")\n",
        "\n",
        "    logging.info(f\"Found {len(pdf_files)} PDF file(s) in '{pdf_folder}'.\")\n",
        "    selected_file_path = None\n",
        "\n",
        "    if mode == \"latest\":\n",
        "        try:\n",
        "            latest_file = max(pdf_files, key=lambda f: os.path.getmtime(os.path.join(pdf_folder, f)))\n",
        "            selected_file_path = os.path.join(pdf_folder, latest_file)\n",
        "            logging.info(f\"Selected latest PDF: {latest_file}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error determining latest file: {e}\")\n",
        "            raise\n",
        "    elif mode == \"random\":\n",
        "        random_file = random.choice(pdf_files)\n",
        "        selected_file_path = os.path.join(pdf_folder, random_file)\n",
        "        logging.info(f\"Selected random PDF: {random_file}\")\n",
        "    elif mode == \"interactive\":\n",
        "        # (Interactive code omitted for brevity, can be added back if needed)\n",
        "        logging.warning(\"Interactive mode selection not fully implemented in this snippet.\")\n",
        "        # Fallback to random if interactive part is omitted\n",
        "        random_file = random.choice(pdf_files)\n",
        "        selected_file_path = os.path.join(pdf_folder, random_file)\n",
        "        logging.info(f\"Selected random PDF (fallback): {random_file}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: '{mode}'. Must be 'latest', 'random', or 'interactive'\")\n",
        "\n",
        "    if selected_file_path is None: raise RuntimeError(\"Failed to select a PDF file.\")\n",
        "    return selected_file_path\n",
        "\n",
        "\n",
        "def extract_full_text_metadata_pypdf(pdf_path: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Extracts the full text content and metadata from a PDF using pypdf.\n",
        "    (Standalone version)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        logging.error(f\"PDF file not found: {pdf_path}\")\n",
        "        return None, None\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        logging.error(f\"Path exists but is not a file: {pdf_path}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Opening PDF: {os.path.basename(pdf_path)} using pypdf\")\n",
        "        reader = PdfReader(pdf_path)\n",
        "        metadata = reader.metadata\n",
        "        num_pages = len(reader.pages)\n",
        "\n",
        "        extracted_metadata = {\n",
        "            \"title\": getattr(metadata, 'title', os.path.basename(pdf_path)),\n",
        "            \"author\": getattr(metadata, 'author', \"Unknown\"),\n",
        "            \"subject\": getattr(metadata, 'subject', \"\"),\n",
        "            \"creator\": getattr(metadata, 'creator', \"\"),\n",
        "            \"producer\": getattr(metadata, 'producer', \"\"),\n",
        "            \"page_count\": num_pages,\n",
        "            \"is_encrypted\": reader.is_encrypted,\n",
        "            \"file_name\": os.path.basename(pdf_path),\n",
        "            \"file_path\": pdf_path\n",
        "        }\n",
        "\n",
        "        logging.info(f\"Extracting text from {num_pages} page(s)...\")\n",
        "        full_text = \"\"\n",
        "        for page_num, page in enumerate(reader.pages):\n",
        "            try:\n",
        "                 text = page.extract_text()\n",
        "                 if text:\n",
        "                     full_text += text + \"\\n\" # Add single newline between pages\n",
        "                 else:\n",
        "                      logging.warning(f\"No text extracted from page {page_num + 1}.\")\n",
        "            except Exception as page_error:\n",
        "                 logging.warning(f\"Could not extract text from page {page_num + 1}: {page_error}\")\n",
        "\n",
        "        logging.info(f\"Successfully extracted {len(full_text)} characters from PDF.\")\n",
        "        full_text = full_text.replace('\\x00', '').strip() # Remove null chars and trim ends\n",
        "\n",
        "        # Simple preprocessing: fix hyphenation and normalize whitespace\n",
        "        full_text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', full_text)\n",
        "        full_text = re.sub(r'\\s+', ' ', full_text) # Normalize all whitespace to single space\n",
        "\n",
        "        return full_text, extracted_metadata\n",
        "\n",
        "    except ImportError:\n",
        "         logging.error(\"pypdf library is required but not installed.\")\n",
        "         return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF '{os.path.basename(pdf_path)}' using pypdf: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- EDA Functions (Operating on Full Text) ---\n",
        "\n",
        "STOP_WORDS = set([\n",
        "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        "    'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
        "    'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
        "    'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
        "    'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
        "    \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
        "    'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
        "    'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such',\n",
        "    'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too',\n",
        "    'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
        "    'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves',\n",
        "    # Domain specific\n",
        "    'et', 'al', 'nber', 'working', 'paper', 'series', 'figure', 'table', 'http', 'org', 'www', 'abstract', 'doi', 'appendix',\n",
        "    'university', 'research', 'data', 'results', 'analysis', 'study', 'based', 'using', 'also', 'however', 'within', 'whether'\n",
        "])\n",
        "\n",
        "def calculate_basic_stats(full_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Calculates basic statistics from the full text.\"\"\"\n",
        "    stats = {}\n",
        "    if not full_text: return {\"error\": \"Input text is empty\"}\n",
        "    try:\n",
        "        words = full_text.split()\n",
        "        stats['word_count'] = len(words)\n",
        "        # Simple sentence split - may be inaccurate with abbreviations etc.\n",
        "        sentences = re.split(r'[.?!]\\s+', full_text) # Split on .?! followed by space\n",
        "        stats['sentence_count'] = len([s for s in sentences if s.strip()])\n",
        "        if stats['sentence_count'] > 0:\n",
        "            stats['avg_sentence_length_words'] = round(stats['word_count'] / stats['sentence_count'], 2)\n",
        "        else:\n",
        "            stats['avg_sentence_length_words'] = stats['word_count']\n",
        "        stats['character_count'] = len(full_text)\n",
        "        logging.info(\"Calculated basic text statistics.\")\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating basic stats: {e}\")\n",
        "        return {\"error\": f\"Failed to calculate stats: {e}\"}\n",
        "\n",
        "def get_word_frequency(full_text: str, num_words: int = 25) -> List[Tuple[str, int]]:\n",
        "    \"\"\"Calculates frequency of significant words in the full text.\"\"\"\n",
        "    if not full_text: return []\n",
        "    try:\n",
        "        text = full_text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        text = re.sub(r'\\d+', '', text) # Remove numbers\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in STOP_WORDS and len(word) > 2]\n",
        "        word_counts = Counter(filtered_words)\n",
        "        most_common = word_counts.most_common(num_words)\n",
        "        logging.info(f\"Calculated word frequencies, found top {len(most_common)} words.\")\n",
        "        return most_common\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating word frequency: {e}\")\n",
        "        return []\n",
        "\n",
        "def find_potential_proper_nouns(full_text: str, min_freq: int = 3) -> List[Tuple[str, int]]:\n",
        "    \"\"\"Finds potential proper nouns using capitalization heuristic.\"\"\"\n",
        "    if not full_text: return []\n",
        "    potential_nouns = Counter()\n",
        "    try:\n",
        "        # Use regex to find capitalized words that are not at the start of a line\n",
        "        # This is still a very rough heuristic.\n",
        "        # Matches words starting with an uppercase letter, followed by lowercase,\n",
        "        # NOT preceded by sentence-ending punctuation and space, or start of text.\n",
        "        pattern = r\"(?<![\\.\\?!]\\s)(?<!^)\\b([A-Z][a-z]+(?:[-'][A-Z][a-z]+)*)\\b\"\n",
        "        matches = re.findall(pattern, full_text)\n",
        "        potential_nouns.update(matches)\n",
        "\n",
        "        frequent_nouns = [(noun, freq) for noun, freq in potential_nouns.items() if freq >= min_freq and noun.lower() not in STOP_WORDS]\n",
        "        frequent_nouns.sort(key=lambda x: x[1], reverse=True)\n",
        "        logging.info(f\"Found {len(frequent_nouns)} potential proper nouns (heuristic) with min frequency {min_freq}.\")\n",
        "        return frequent_nouns\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error finding potential proper nouns: {e}\")\n",
        "        return []\n",
        "\n",
        "def find_common_headers(full_text: str) -> Dict[str, int]:\n",
        "    \"\"\"Counts occurrences of common academic paper headers.\"\"\"\n",
        "    if not full_text: return {}\n",
        "    headers = {\n",
        "        \"Abstract\": 0, \"Introduction\": 0, \"Method\": 0, \"Methodology\": 0,\n",
        "        \"Data\": 0, \"Results\": 0, \"Discussion\": 0, \"Conclusion\": 0,\n",
        "        \"References\": 0, \"Appendix\": 0\n",
        "    }\n",
        "    found_headers = {}\n",
        "    try:\n",
        "        # Case-insensitive search for headers at the start of a line (potentially with numbers/whitespace)\n",
        "        for header in headers.keys():\n",
        "             # Regex: start of line, optional whitespace/numbering, header text, optional colon, whitespace/newline end\n",
        "            pattern = re.compile(r\"^\\s*(?:\\d+\\.?\\s*)?\" + re.escape(header) + r\"\\s*:?\\s*$\", re.IGNORECASE | re.MULTILINE)\n",
        "            matches = pattern.findall(full_text)\n",
        "            count = len(matches)\n",
        "            if count > 0:\n",
        "                found_headers[header] = count\n",
        "        logging.info(f\"Checked for common headers. Found: {found_headers}\")\n",
        "        return found_headers\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error finding common headers: {e}\")\n",
        "        return {}\n",
        "\n",
        "# --- Main EDA Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"Standalone EDA Script execution started.\")\n",
        "\n",
        "    # <<< --- CONFIGURATION --- >>>\n",
        "    PDF_FOLDER = \"my_pdfs\"  # IMPORTANT: Change this\n",
        "    SELECTION_MODE = \"random\" # \"latest\", \"random\" (interactive needs more code)\n",
        "    TOP_N_WORDS = 30\n",
        "    MIN_PROPER_NOUN_FREQ = 4\n",
        "    # <<< --- END CONFIGURATION --- >>>\n",
        "\n",
        "    try:\n",
        "        # Step 1: Select PDF\n",
        "        pdf_path = get_pdf_file(PDF_FOLDER, mode=SELECTION_MODE)\n",
        "\n",
        "        # Step 2: Extract Full Text & Metadata\n",
        "        full_text, metadata = extract_full_text_metadata_pypdf(pdf_path)\n",
        "\n",
        "        if full_text and metadata:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"Running Standalone EDA for: {metadata.get('file_name', 'N/A')}\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            # Step 3: Basic Stats Analysis\n",
        "            print(\"\\n--- Basic Text Statistics ---\")\n",
        "            stats = calculate_basic_stats(full_text)\n",
        "            if 'error' in stats: print(f\"  Error: {stats['error']}\")\n",
        "            else:\n",
        "                for key, value in stats.items(): print(f\"  {key.replace('_', ' ').capitalize():<30}: {value}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            # Step 4: Word Frequency (Potential Topics)\n",
        "            print(f\"\\n--- Top {TOP_N_WORDS} Frequent Words (Potential Topics) ---\")\n",
        "            top_words = get_word_frequency(full_text, num_words=TOP_N_WORDS)\n",
        "            if top_words:\n",
        "                col_width = 20\n",
        "                for i in range(0, len(top_words), 2):\n",
        "                     word1, freq1 = top_words[i]\n",
        "                     entry1 = f\"{word1}: {freq1}\"\n",
        "                     entry2 = \"\"\n",
        "                     if i + 1 < len(top_words):\n",
        "                         word2, freq2 = top_words[i+1]\n",
        "                         entry2 = f\"{word2}: {freq2}\"\n",
        "                     print(f\"  {entry1:<{col_width}} {entry2:<{col_width}}\")\n",
        "            else: print(\"  Could not calculate word frequencies.\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            # Step 5: Potential Proper Nouns (Characters/Entities - HEURISTIC)\n",
        "            print(f\"\\n--- Potential Proper Nouns (Frequency >= {MIN_PROPER_NOUN_FREQ}) ---\")\n",
        "            print(\"  (Warning: Basic heuristic, may include errors. Use NER for accuracy)\")\n",
        "            potential_names = find_potential_proper_nouns(full_text, min_freq=MIN_PROPER_NOUN_FREQ)\n",
        "            if potential_names:\n",
        "                col_width = 25\n",
        "                for i in range(0, len(potential_names), 2):\n",
        "                     name1, freq1 = potential_names[i]\n",
        "                     entry1 = f\"{name1}: {freq1}\"\n",
        "                     entry2 = \"\"\n",
        "                     if i + 1 < len(potential_names):\n",
        "                         name2, freq2 = potential_names[i+1]\n",
        "                         entry2 = f\"{name2}: {freq2}\"\n",
        "                     print(f\"  {entry1:<{col_width}} {entry2:<{col_width}}\")\n",
        "            else: print(f\"  No potential proper nouns found with frequency >= {MIN_PROPER_NOUN_FREQ}.\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            # Step 6: Common Header Check\n",
        "            print(\"\\n--- Common Header Check ---\")\n",
        "            found_headers = find_common_headers(full_text)\n",
        "            if found_headers:\n",
        "                 for header, count in found_headers.items(): print(f\"  Found '{header}': {count} time(s)\")\n",
        "            else: print(\"  No common headers (Abstract, Introduction, etc.) found matching patterns.\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        else:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"EDA Failed: Could not extract text from the selected PDF.\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"File/Folder Error: {e}\")\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Configuration or Input Error: {e}\")\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
        "        print(f\"\\nUNEXPECTED ERROR: {e}\")\n",
        "\n",
        "    logging.info(\"Standalone EDA Script execution finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noIm0SVc_Rxl",
        "outputId": "339031d5-14b9-4545-c0b2-d59cf65e9e70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Running Standalone EDA for: w27392.pdf\n",
            "============================================================\n",
            "\n",
            "--- Basic Text Statistics ---\n",
            "  Word count                    : 9930\n",
            "  Sentence count                : 399\n",
            "  Avg sentence length words     : 24.89\n",
            "  Character count               : 62068\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Top 30 Frequent Words (Potential Topics) ---\n",
            "  covid: 135           students: 117       \n",
            "  eﬀects: 63           outcomes: 55        \n",
            "  ∗∗∗: 55              health: 51          \n",
            "  treatment: 48        economic: 37        \n",
            "  online: 35           due: 34             \n",
            "  graduation: 34       job: 34             \n",
            "  student: 31          likely: 31          \n",
            "  income: 28           proxies: 27         \n",
            "  survey: 26           eﬀect: 25           \n",
            "  honors: 25           average: 25         \n",
            "  pandemic: 24         expected: 24        \n",
            "  lost: 24             state: 22           \n",
            "  academic: 22         sample: 22          \n",
            "  expectations: 20     shocks: 20          \n",
            "  major: 20            gpa: 20             \n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Potential Proper Nouns (Frequency >= 4) ---\n",
            "  (Warning: Basic heuristic, may include errors. Use NER for accuracy)\n",
            "  Honors: 27                Health: 14               \n",
            "  Proxies: 14               Lost: 13                 \n",
            "  Likelihood: 12            Economic: 11             \n",
            "  Student: 11               Time: 11                 \n",
            "  Panel: 10                 Family: 9                \n",
            "  Subjective: 8             Job: 8                   \n",
            "  Notes: 8                  Income: 8                \n",
            "  Prob: 8                   Zafar: 7                 \n",
            "  Economics: 7              Arizona: 7               \n",
            "  State: 7                  Treatment: 7             \n",
            "  Change: 7                 Principal: 6             \n",
            "  Component: 6              Catch: 6                 \n",
            "  Jacob: 5                  Survey: 5                \n",
            "  Earnings: 5               Esteban: 4               \n",
            "  Maria: 4                  Paola: 4                 \n",
            "  Ugalde: 4                 Araya: 4                 \n",
            "  Basit: 4                  April: 4                 \n",
            "  March: 4                  Semester: 4              \n",
            "  May: 4                    Journal: 4               \n",
            "  Major: 4                  Prop: 4                  \n",
            "  Mean: 4                   High: 4                  \n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Common Header Check ---\n",
            "  No common headers (Abstract, Introduction, etc.) found matching patterns.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sA9aR7dyDHxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}